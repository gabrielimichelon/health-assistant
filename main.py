import os
import json
import tempfile
import streamlit as st
from datetime import datetime
from dotenv import load_dotenv

# Ãudio
from audio_recorder_streamlit import audio_recorder

# OpenAI (Whisper + Chat)
from openai import OpenAI

# LangChain + Qdrant
from typing import List, Optional
from langchain.schema import Document
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Qdrant
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings
from qdrant_client import QdrantClient
from langchain_qdrant import QdrantVectorStore


# -----------------------------------------------------
# CONFIGURAÃ‡Ã•ES INICIAIS
# -----------------------------------------------------

load_dotenv()
QDRANT_API_KEY=os.getenv("QDRANT_API_KEY")
QDRANT_URL=os.getenv("QDRANT_URL")
QDRANT_COLLECTION_NAME=os.getenv("QDRANT_COLLECTION_NAME")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=OPENAI_API_KEY)

st.set_page_config(page_title="Assistente de Bem-Estar", page_icon="ğŸŒ¿")
st.title("Health Assistant â€” Seu especialista em bem-estar natural")

st.caption("Ã‰ possÃ­vel melhorar sua qualidade de vida com recursos naturais e hÃ¡bitos saudÃ¡veis. Estou aqui para te ajudar nesse processo contribuindo para uma vida mais saudÃ¡vel!  ")

# -----------------------------------------------------
# MEMÃ“RIA PERSISTENTE
# -----------------------------------------------------

MEMORY_FILE = "memory.json"

def load_memory():
    if not os.path.exists(MEMORY_FILE):
        return []
    with open(MEMORY_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

def save_memory(memory):
    with open(MEMORY_FILE, "w", encoding="utf-8") as f:
        json.dump(memory, f, indent=2, ensure_ascii=False)

def add_memory_entry(user_message):
    """Extrai sintomas e salva com a data."""
    llm_extract = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    extract_prompt = f"""
    Extraia sintomas mencionados no texto abaixo e retorne como lista JSON.

    Texto: "{user_message}"

    Responda apenas no formato:
    ["sintoma1", "sintoma2", ...]
    """

    try:
        extracted = llm_extract.invoke(extract_prompt).content
        symptoms = json.loads(extracted)
    except:
        symptoms = []

    if symptoms:
        memory = load_memory()
        memory.append({
            "date": datetime.now().strftime("%Y-%m-%d %H:%M"),
            "symptoms": symptoms,
            "text": user_message
        })
        save_memory(memory)


# -----------------------------------------------------
# RAG SETUP
# -----------------------------------------------------

# loader = TextLoader("instruct.txt")
# documents = loader.load()

# splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
# docs = splitter.split_documents(documents)

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2"
)

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)

prompt = ChatPromptTemplate.from_template(
    """
VocÃª Ã© um assistente especializado em bem-estar natural.

MEMÃ“RIA DO USUÃRIO (HISTÃ“RICO DE SINTOMAS):
{user_memory}

CONTEXTOS RECUPERADOS DO RAG:
{context}

INSTRUÃ‡Ã•ES:
- Utilize o histÃ³rico quando relevante.
- Utilize o contexto tÃ©cnico do RAG quando necessÃ¡rio.
- Comente sobre recorrÃªncia de sintomas quando aplicÃ¡vel.
- NÃ£o invente informaÃ§Ãµes.
- Se nÃ£o houver dados suficientes, diga isso claramente.

Pergunta:
{question}
    """
)

def search_similar_documents(
    query: str,
    embedding_model: HuggingFaceEmbeddings,
    k: int = 3
) -> List[Document]:
    """
    Busca documentos similares no QDrant.
    
    Args:
        query: Texto de consulta
        embedding_model: Modelo de embedding (padrÃ£o: OpenAIEmbeddings)
        k: NÃºmero de resultados a retornar
    
    Returns:
        List[Document]: Lista de documentos similares
    """
    
    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

    vector_store = QdrantVectorStore(
        client=client,
        collection_name=QDRANT_COLLECTION_NAME,
        embedding=embedding_model,
    )
    
    results= vector_store.similarity_search_with_relevance_scores(query=query, k=k)
    return results


def rag_pipeline(question):
    memory = load_memory()

    memory_text = "\n".join(
        [f"- {m['date']}: {m['text']} (sintomas: {', '.join(m['symptoms'])})"
         for m in memory[-2:]]
    )
    # memory_text=str(st.session_state.chat_history[-4:]) # Ãºltimas 2 conversas

    qdocs = search_similar_documents(query=question, embedding_model=embeddings)
    context = "\n\n".join([doc[0].page_content for doc in qdocs])

    chain_input = {
        "context": context,
        "question": question,
        "user_memory": memory_text if memory_text else "Sem histÃ³rico armazenado."
    }

    response = llm.invoke(prompt.format(**chain_input))
    return response.content


# -----------------------------------------------------
# INTERFACE (TEXTO + ÃUDIO)
# -----------------------------------------------------

# st.divider()

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Create a placeholder
placeholder = st.empty()
 
with st.container():
    audio_bytes = audio_recorder(pause_threshold=2.0, sample_rate=41_000, text="",
        recording_color="#e8352c",
        neutral_color="#6aa36f",
        icon_name="microphone",
        icon_size="1x",)

    user_input = st.chat_input("Como posso ajudar hoje?") 




# -----------------------------------------------------
# ENTRADA POR TEXTO
# -----------------------------------------------------

if user_input:
    with placeholder.container(height=550):
        if st.session_state.chat_history:
            for speaker, msg in st.session_state.chat_history:
                st.chat_message("user" if speaker.startswith("VocÃª") else "assistant", avatar= "ğŸ¤·" if speaker.startswith("VocÃª") else "ğŸ‘©â€ğŸŒ¾").markdown(msg)

    with st.spinner("Analisando..."):
        add_memory_entry(user_input)
        resposta = rag_pipeline(user_input)

    st.session_state.chat_history.append(("VocÃª", user_input))
    st.session_state.chat_history.append(("Assistente", resposta))


# -----------------------------------------------------
# ENTRADA POR ÃUDIO
# -----------------------------------------------------

if audio_bytes:
    with placeholder.container(height=550):
        if st.session_state.chat_history:
            for speaker, msg in st.session_state.chat_history:
                st.chat_message("user" if speaker.startswith("VocÃª") else "assistant", avatar= "ğŸ¤·" if speaker.startswith("VocÃª") else "ğŸ‘©â€ğŸŒ¾").markdown(msg)
    # st.success("Ouvindo...")

    with st.spinner("Aguarde..."):
        # salvar temporÃ¡rio
        temp_audio = "temp_audio.wav"
        with open(temp_audio, "wb") as f:
            f.write(audio_bytes)

        with open(temp_audio, "rb") as f:
            transcription = client.audio.transcriptions.create(
                model="whisper-1",
                file=f
            )

    user_text = transcription.text
    st.write(f" **{user_text}**")

    with st.spinner("Analisando..."):
        add_memory_entry(user_text)
        resposta = rag_pipeline(user_text)

    st.session_state.chat_history.append(("VocÃª (Ã¡udio)", user_text))
    st.session_state.chat_history.append(("Assistente", resposta))


# -----------------------------------------------------
# HISTÃ“RICO DO CHAT
# -----------------------------------------------------
with placeholder.container(height=550):
    if st.session_state.chat_history:
        for speaker, msg in st.session_state.chat_history:
            st.chat_message("user" if speaker.startswith("VocÃª") else "assistant", avatar= "ğŸ¤·" if speaker.startswith("VocÃª") else "ğŸ‘©â€ğŸŒ¾").markdown(msg)
    else:
        st.image("health-clipart.png", width=650)
    