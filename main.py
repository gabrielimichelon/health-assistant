import os
import json
import tempfile
import streamlit as st
from datetime import datetime
from dotenv import load_dotenv

# Ãudio
from audio_recorder_streamlit import audio_recorder

# OpenAI (Whisper + Chat)
from openai import OpenAI

# LangChain + Qdrant - IMPORTS CORRIGIDOS
from typing import List, Optional
from langchain_core.documents import Document  # MUDANÃ‡A AQUI
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Qdrant
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings
from qdrant_client import QdrantClient
from langchain_qdrant import QdrantVectorStore

# Importar o avaliador
from llm_evaluator import LLMEvaluator

# -----------------------------------------------------
# CONFIGURAÃ‡Ã•ES INICIAIS
# -----------------------------------------------------

load_dotenv()
QDRANT_API_KEY=os.getenv("QDRANT_API_KEY")
QDRANT_URL=os.getenv("QDRANT_URL")
QDRANT_COLLECTION_NAME=os.getenv("QDRANT_COLLECTION_NAME")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=OPENAI_API_KEY)

# Inicializar avaliador
evaluator = LLMEvaluator()

st.set_page_config(page_title="Assistente de Bem-Estar", page_icon="ğŸŒ¿")
st.title("Health Assistant â€” Seu especialista em bem-estar natural")

st.caption("Ã‰ possÃ­vel melhorar sua qualidade de vida com recursos naturais e hÃ¡bitos saudÃ¡veis. Estou aqui para te ajudar nesse processo contribuindo para uma vida mais saudÃ¡vel!  ")

# -----------------------------------------------------
# MEMÃ“RIA PERSISTENTE
# -----------------------------------------------------

MEMORY_FILE = "memory.json"

def load_memory():
    if not os.path.exists(MEMORY_FILE):
        return []
    with open(MEMORY_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

def save_memory(memory):
    with open(MEMORY_FILE, "w", encoding="utf-8") as f:
        json.dump(memory, f, indent=2, ensure_ascii=False)

def add_memory_entry(user_message):
    """Extrai sintomas e salva com a data."""
    llm_extract = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    extract_prompt = f"""
    Extraia sintomas mencionados no texto abaixo e retorne como lista JSON.

    Texto: "{user_message}"

    Responda apenas no formato:
    ["sintoma1", "sintoma2", ...]
    """

    try:
        extracted = llm_extract.invoke(extract_prompt).content
        symptoms = json.loads(extracted)
    except:
        symptoms = []

    if symptoms:
        memory = load_memory()
        memory.append({
            "date": datetime.now().strftime("%Y-%m-%d %H:%M"),
            "symptoms": symptoms,
            "text": user_message
        })
        save_memory(memory)


# -----------------------------------------------------
# RAG SETUP
# -----------------------------------------------------

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2"
)

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)

# prompt = ChatPromptTemplate.from_template(
#     """
# VocÃª Ã© um assistente especializado em bem-estar natural.

# MEMÃ“RIA DO USUÃRIO (HISTÃ“RICO DE SINTOMAS):
# {user_memory}

# CONTEXTOS RECUPERADOS DO RAG:
# {context}

# INSTRUÃ‡Ã•ES:
# - Utilize o histÃ³rico quando relevante.
# - Utilize o contexto tÃ©cnico do RAG quando necessÃ¡rio.
# - Comente sobre recorrÃªncia de sintomas quando aplicÃ¡vel.
# - NÃ£o invente informaÃ§Ãµes.
# - Se nÃ£o houver dados suficientes, diga isso claramente.

# Pergunta:
# {question}
#     """
# )

prompt = ChatPromptTemplate.from_template(
    """
VocÃª Ã© um assistente especializado em bem-estar natural e prÃ¡ticas integrativas de saÃºde.

MEMÃ“RIA DO USUÃRIO (HISTÃ“RICO DE SINTOMAS):
{user_memory}

CONTEXTOS RECUPERADOS DO RAG:
{context}

INSTRUÃ‡Ã•ES:
- Utilize o histÃ³rico quando relevante.
- Utilize o contexto tÃ©cnico do RAG quando necessÃ¡rio.
- Comente sobre recorrÃªncia de sintomas quando aplicÃ¡vel.
- NÃ£o invente informaÃ§Ãµes.
- Se nÃ£o houver dados suficientes, diga isso claramente.

INSTRUÃ‡Ã•ES DA RESPOSTA:

1. ANÃLISE DE HISTÃ“RICO:
   - Identifique padrÃµes e recorrÃªncias de sintomas
   - Mencione explicitamente quando houver sintomas repetidos
   - Considere a frequÃªncia e duraÃ§Ã£o dos sintomas relatados

2. USO DO CONHECIMENTO TÃ‰CNICO:
   - Base suas recomendaÃ§Ãµes EXCLUSIVAMENTE no contexto fornecido
   - Cite as fontes quando mencionar informaÃ§Ãµes tÃ©cnicas
   - Use linguagem acessÃ­vel para explicar conceitos complexos

3. SEGURANÃ‡A E RESPONSABILIDADE:
   - NUNCA substitua orientaÃ§Ã£o mÃ©dica profissional
   - Recomende buscar um profissional de saÃºde para sintomas graves, persistentes ou preocupantes
   - Deixe claro quando uma informaÃ§Ã£o estÃ¡ alÃ©m do seu escopo
   - NÃ£o faÃ§a diagnÃ³sticos ou prescreva tratamentos

4. QUALIDADE DA RESPOSTA:
   - Seja especÃ­fico e prÃ¡tico nas recomendaÃ§Ãµes
   - Organize a resposta em tÃ³picos quando apropriado
   - Inclua contraindicaÃ§Ãµes e precauÃ§Ãµes relevantes
   - Se nÃ£o houver informaÃ§Ãµes suficientes, admita claramente

5. TOM E ESTILO:
   - Seja empÃ¡tico e acolhedor
   - Use linguagem clara e objetiva
   - Evite jargÃµes mÃ©dicos sem explicaÃ§Ã£o
   - Demonstre cuidado genuÃ­no com o bem-estar do usuÃ¡rio

Pergunta:
{question}
"""
)

def search_similar_documents(
    query: str,
    embedding_model: HuggingFaceEmbeddings,
    k: int = 3
) -> List[Document]:
    """
    Busca documentos similares no QDrant.
    
    Args:
        query: Texto de consulta
        embedding_model: Modelo de embedding (padrÃ£o: OpenAIEmbeddings)
        k: NÃºmero de resultados a retornar
    
    Returns:
        List[Document]: Lista de documentos similares
    """
    
    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

    vector_store = QdrantVectorStore(
        client=client,
        collection_name=QDRANT_COLLECTION_NAME,
        embedding=embedding_model,
    )
    
    results= vector_store.similarity_search_with_relevance_scores(query=query, k=k)
    return results


def rag_pipeline(question):
    memory = load_memory()

    memory_text = "\n".join(
        [f"- {m['date']}: {m['text']} (sintomas: {', '.join(m['symptoms'])})"
         for m in memory[-2:]]
    )

    qdocs = search_similar_documents(query=question, embedding_model=embeddings)
    context = "\n\n".join([doc[0].page_content for doc in qdocs])

    chain_input = {
        "context": context,
        "question": question,
        "user_memory": memory_text if memory_text else "Sem histÃ³rico armazenado."
    }
    
    print("CHAIN INPUT:", chain_input)

    response = llm.invoke(prompt.format(**chain_input))
    
    # AVALIAÃ‡ÃƒO COM RAGAS (sem ground_truth)
    contexts_for_eval = [doc[0].page_content for doc in qdocs]
    metrics = evaluator.evaluate_response(
        question=question,
        llm_answer=response.content,
        contexts=contexts_for_eval
    )
    
    # Salvar mÃ©tricas
    metrics["question"] = question
    metrics["answer"] = response.content
    evaluator.save_metrics(metrics)
    
    return response.content, metrics


# -----------------------------------------------------
# INTERFACE (TEXTO + ÃUDIO)
# -----------------------------------------------------

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "show_metrics" not in st.session_state:
    st.session_state.show_metrics = False

# Create a placeholder
placeholder = st.empty()
 
with st.container():
    audio_bytes = audio_recorder(pause_threshold=2.0, sample_rate=41_000, text="",
        recording_color="#e8352c",
        neutral_color="#6aa36f",
        icon_name="microphone",
        icon_size="1x",)

    user_input = st.chat_input("Como posso ajudar hoje?") 

# Sidebar com mÃ©tricas
with st.sidebar:
    st.header("ğŸ“Š MÃ©tricas de Qualidade")
    
    # Obter Ãºltimas mÃ©tricas do histÃ³rico
    last_metrics = None
    if st.session_state.chat_history:
        for speaker, msg, metrics in reversed(st.session_state.chat_history):
            if metrics:
                last_metrics = metrics
                break
    
    if last_metrics:
        st.subheader("Ãšltima resposta:")
        
        # Score composto
        composite = last_metrics.get("composite_score", 0)
        quality = last_metrics.get("quality_rating", "N/A")
        st.metric("ğŸ“Š Score Geral", f"{composite:.2f}", quality)
        
        st.divider()
        
        # MÃ©tricas RAGAS
        st.markdown("**ğŸ” MÃ©tricas RAGAS**")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ğŸ“ RelevÃ¢ncia", f"{last_metrics.get('answer_relevancy', 0):.2f}")
        with col2:
            st.metric("âœ… Fidelidade", f"{last_metrics.get('faithfulness', 0):.2f}")
        
        st.divider()
        
        # MÃ©tricas de SaÃºde
        st.markdown("**ğŸ¥ MÃ©tricas de SaÃºde**")
        col3, col4 = st.columns(2)
        with col3:
            st.metric("ğŸ›¡ï¸ SeguranÃ§a", f"{last_metrics.get('safety', 0):.2f}")
            st.metric("ğŸ“‹ Completude", f"{last_metrics.get('completeness', 0):.2f}")
            st.metric("ğŸ“š FundamentaÃ§Ã£o", f"{last_metrics.get('source_attribution', 0):.2f}")
        with col4:
            st.metric("ğŸ¯ PrecisÃ£o", f"{last_metrics.get('medical_accuracy', 0):.2f}")
            st.metric("âš¡ Acionabilidade", f"{last_metrics.get('actionability', 0):.2f}")
        
        # Mostrar issues crÃ­ticas se existirem
        critical_issues = last_metrics.get('critical_issues', [])
        if critical_issues:
            st.warning("âš ï¸ **Problemas CrÃ­ticos Detectados:**")
            for issue in critical_issues:
                st.write(f"- {issue}")
        
        st.divider()
        
        # BotÃ£o para gerar relatÃ³rio
        if st.button("ğŸ“Š Gerar RelatÃ³rio Completo"):
            report = evaluator.generate_report()
            if report:
                st.json(report)
                
        # DEBUG: Mostrar todas as mÃ©tricas disponÃ­veis
        with st.expander("ğŸ” Ver todas as mÃ©tricas (DEBUG)"):
            st.json(last_metrics)
    else:
        st.info("ğŸ’¬ FaÃ§a uma pergunta para ver as mÃ©tricas de qualidade")

# -----------------------------------------------------
# ENTRADA POR TEXTO
# -----------------------------------------------------

if user_input:
    # Adicionar mensagem do usuÃ¡rio imediatamente
    st.session_state.chat_history.append(("VocÃª", user_input, None))
    
    # Atualizar display
    with placeholder.container(height=550):
        for speaker, msg, metrics in st.session_state.chat_history:
            st.chat_message("user" if speaker.startswith("VocÃª") else "assistant", 
                          avatar="ğŸ¤·" if speaker.startswith("VocÃª") else "ğŸ‘©â€ğŸŒ¾").markdown(msg)

    with st.spinner("Analisando..."):
        add_memory_entry(user_input)
        resposta, metrics = rag_pipeline(user_input)

    # Adicionar resposta com mÃ©tricas
    st.session_state.chat_history.append(("Assistente", resposta, metrics))
    
    # ForÃ§ar rerun para atualizar sidebar
    st.rerun()

# -----------------------------------------------------
# ENTRADA POR ÃUDIO
# -----------------------------------------------------

if audio_bytes:
    with st.spinner("Transcrevendo Ã¡udio..."):
        # salvar temporÃ¡rio
        temp_audio = "temp_audio.wav"
        with open(temp_audio, "wb") as f:
            f.write(audio_bytes)

        with open(temp_audio, "rb") as f:
            transcription = client.audio.transcriptions.create(
                model="whisper-1",
                file=f
            )

    user_text = transcription.text
    
    # Adicionar mensagem do usuÃ¡rio
    st.session_state.chat_history.append(("VocÃª (Ã¡udio)", user_text, None))
    
    # Atualizar display
    with placeholder.container(height=550):
        for speaker, msg, metrics in st.session_state.chat_history:
            st.chat_message("user" if speaker.startswith("VocÃª") else "assistant",
                          avatar="ğŸ¤·" if speaker.startswith("VocÃª") else "ğŸ‘©â€ğŸŒ¾").markdown(msg)

    with st.spinner("Analisando..."):
        add_memory_entry(user_text)
        resposta, metrics = rag_pipeline(user_text)

    # Adicionar resposta com mÃ©tricas
    st.session_state.chat_history.append(("Assistente", resposta, metrics))
    
    # ForÃ§ar rerun para atualizar sidebar
    st.rerun()


# -----------------------------------------------------
# HISTÃ“RICO DO CHAT
# -----------------------------------------------------
with placeholder.container(height=550):
    if st.session_state.chat_history:
        for speaker, msg, metrics in st.session_state.chat_history:
            st.chat_message("user" if speaker.startswith("VocÃª") else "assistant", avatar= "ğŸ¤·" if speaker.startswith("VocÃª") else "ğŸ‘©â€ğŸŒ¾").markdown(msg)
    else:
        st.image("health-clipart.png", width=650)
